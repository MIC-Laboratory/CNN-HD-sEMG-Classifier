{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of depolying CNN gensture reconition model to edge device (e.g. Sony Spresense)\n",
    "\n",
    "Step Overview:\n",
    "1. Conver Pytorch Model to Onnx Model\n",
    "2. Conver Onnx Model to Keras Model\n",
    "3. Conver Keras Model to quantization aware model\n",
    "4. Retraining quantization aware model\n",
    "5. Convert it to Tensorflow Lite Model\n",
    "6. Using Edge Impuls to generate code for Sony Spresense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conver Pytorch Model to Onnx Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input.1 : Float(1, 1, 8, 24, strides=[192, 192, 24, 1], requires_grad=0, device=cpu),\n",
      "      %fc.weight : Float(8, 128, strides=[128, 1], requires_grad=1, device=cpu),\n",
      "      %fc.bias : Float(8, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_125 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_126 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_128 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_129 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_131 : Float(32, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_132 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_134 : Float(32, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_135 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_137 : Float(64, 32, 1, 1, strides=[32, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_138 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_140 : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_141 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_143 : Float(64, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_144 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_146 : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_147 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_149 : Float(64, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_150 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_152 : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_153 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_155 : Float(64, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_156 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_158 : Float(64, 1, 3, 3, strides=[9, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_159 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_161 : Float(128, 64, 1, 1, strides=[64, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_162 : Float(128, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %/model/model.0/model.0.0/Conv_output_0 : Float(1, 32, 4, 12, strides=[1536, 48, 12, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/model/model.0/model.0.0/Conv\"](%input.1, %onnx::Conv_125, %onnx::Conv_126), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.0/torch.nn.modules.conv.Conv2d::model.0.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.0/model.0.2/Relu_output_0 : Float(1, 32, 4, 12, strides=[1536, 48, 12, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.0/model.0.2/Relu\"](%/model/model.0/model.0.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.0/torch.nn.modules.activation.ReLU::model.0.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.1/model.1.0/Conv_output_0 : Float(1, 32, 4, 12, strides=[1536, 48, 12, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/model.1/model.1.0/Conv\"](%/model/model.0/model.0.2/Relu_output_0, %onnx::Conv_128, %onnx::Conv_129), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.1/torch.nn.modules.conv.Conv2d::model.1.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.1/model.1.2/Relu_output_0 : Float(1, 32, 4, 12, strides=[1536, 48, 12, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.1/model.1.2/Relu\"](%/model/model.1/model.1.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.1/torch.nn.modules.activation.ReLU::model.1.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.1/model.1.3/Conv_output_0 : Float(1, 32, 4, 12, strides=[1536, 48, 12, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/model.1/model.1.3/Conv\"](%/model/model.1/model.1.2/Relu_output_0, %onnx::Conv_131, %onnx::Conv_132), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.1/torch.nn.modules.conv.Conv2d::model.1.3 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.1/model.1.5/Relu_output_0 : Float(1, 32, 4, 12, strides=[1536, 48, 12, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.1/model.1.5/Relu\"](%/model/model.1/model.1.3/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.1/torch.nn.modules.activation.ReLU::model.1.5 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.2/model.2.0/Conv_output_0 : Float(1, 32, 2, 6, strides=[384, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/model/model.2/model.2.0/Conv\"](%/model/model.1/model.1.5/Relu_output_0, %onnx::Conv_134, %onnx::Conv_135), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.2/torch.nn.modules.conv.Conv2d::model.2.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.2/model.2.2/Relu_output_0 : Float(1, 32, 2, 6, strides=[384, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.2/model.2.2/Relu\"](%/model/model.2/model.2.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.2/torch.nn.modules.activation.ReLU::model.2.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.2/model.2.3/Conv_output_0 : Float(1, 64, 2, 6, strides=[768, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/model.2/model.2.3/Conv\"](%/model/model.2/model.2.2/Relu_output_0, %onnx::Conv_137, %onnx::Conv_138), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.2/torch.nn.modules.conv.Conv2d::model.2.3 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.2/model.2.5/Relu_output_0 : Float(1, 64, 2, 6, strides=[768, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.2/model.2.5/Relu\"](%/model/model.2/model.2.3/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.2/torch.nn.modules.activation.ReLU::model.2.5 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.3/model.3.0/Conv_output_0 : Float(1, 64, 2, 6, strides=[768, 12, 6, 1],============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      " requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/model.3/model.3.0/Conv\"](%/model/model.2/model.2.5/Relu_output_0, %onnx::Conv_140, %onnx::Conv_141), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.3/torch.nn.modules.conv.Conv2d::model.3.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.3/model.3.2/Relu_output_0 : Float(1, 64, 2, 6, strides=[768, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.3/model.3.2/Relu\"](%/model/model.3/model.3.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.3/torch.nn.modules.activation.ReLU::model.3.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.3/model.3.3/Conv_output_0 : Float(1, 64, 2, 6, strides=[768, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/model.3/model.3.3/Conv\"](%/model/model.3/model.3.2/Relu_output_0, %onnx::Conv_143, %onnx::Conv_144), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.3/torch.nn.modules.conv.Conv2d::model.3.3 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.3/model.3.5/Relu_output_0 : Float(1, 64, 2, 6, strides=[768, 12, 6, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.3/model.3.5/Relu\"](%/model/model.3/model.3.3/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.3/torch.nn.modules.activation.ReLU::model.3.5 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.4/model.4.0/Conv_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/model/model.4/model.4.0/Conv\"](%/model/model.3/model.3.5/Relu_output_0, %onnx::Conv_146, %onnx::Conv_147), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.4/torch.nn.modules.conv.Conv2d::model.4.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.4/model.4.2/Relu_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.4/model.4.2/Relu\"](%/model/model.4/model.4.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.4/torch.nn.modules.activation.ReLU::model.4.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.4/model.4.3/Conv_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/model.4/model.4.3/Conv\"](%/model/model.4/model.4.2/Relu_output_0, %onnx::Conv_149, %onnx::Conv_150), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.4/torch.nn.modules.conv.Conv2d::model.4.3 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.4/model.4.5/Relu_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.4/model.4.5/Relu\"](%/model/model.4/model.4.3/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.4/torch.nn.modules.activation.ReLU::model.4.5 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.5/model.5.0/Conv_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1], onnx_name=\"/model/model.5/model.5.0/Conv\"](%/model/model.4/model.4.5/Relu_output_0, %onnx::Conv_152, %onnx::Conv_153), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.5/torch.nn.modules.conv.Conv2d::model.5.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.5/model.5.2/Relu_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.5/model.5.2/Relu\"](%/model/model.5/model.5.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.5/torch.nn.modules.activation.ReLU::model.5.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.5/model.5.3/Conv_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/model.5/model.5.3/Conv\"](%/model/model.5/model.5.2/Relu_output_0, %onnx::Conv_155, %onnx::Conv_156), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.5/torch.nn.modules.conv.Conv2d::model.5.3 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.5/model.5.5/Relu_output_0 : Float(1, 64, 1, 3, strides=[192, 3, 3, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.5/model.5.5/Relu\"](%/model/model.5/model.5.3/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.5/torch.nn.modules.activation.ReLU::model.5.5 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.6/model.6.0/Conv_output_0 : Float(1, 64, 1, 2, strides=[128, 2, 2, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2], onnx_name=\"/model/model.6/model.6.0/Conv\"](%/model/model.5/model.5.5/Relu_output_0, %onnx::Conv_158, %onnx::Conv_159), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.6/torch.nn.modules.conv.Conv2d::model.6.0 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.6/model.6.2/Relu_output_0 : Float(1, 64, 1, 2, strides=[128, 2, 2, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.6/model.6.2/Relu\"](%/model/model.6/model.6.0/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.6/torch.nn.modules.activation.ReLU::model.6.2 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.6/model.6.3/Conv_output_0 : Float(1, 128, 1, 2, strides=[256, 2, 2, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1], onnx_name=\"/model/model.6/model.6.3/Conv\"](%/model/model.6/model.6.2/Relu_output_0, %onnx::Conv_161, %onnx::Conv_162), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.6/torch.nn.modules.conv.Conv2d::model.6.3 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/conv.py:459:0\n",
      "  %/model/model.6/model.6.5/Relu_output_0 : Float(1, 128, 1, 2, strides=[256, 2, 2, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"/model/model.6/model.6.5/Relu\"](%/model/model.6/model.6.3/Conv_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.container.Sequential::model.6/torch.nn.modules.activation.ReLU::model.6.5 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1455:0\n",
      "  %/model/model.7/GlobalAveragePool_output_0 : Float(1, 128, 1, 1, strides=[128, 1, 1, 1], requires_grad=1, device=cpu) = onnx::GlobalAveragePool[onnx_name=\"/model/model.7/GlobalAveragePool\"](%/model/model.6/model.6.5/Relu_output_0), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.container.Sequential::model/torch.nn.modules.pooling.AdaptiveAvgPool2d::model.7 # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/functional.py:1214:0\n",
      "  %/Constant_output_0 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=  -1  128 [ CPULongType{2} ], onnx_name=\"/Constant\"](), scope: models.mobilenetv1.MobilenetV1:: # /home/studenta/Documents/CNN-HD-sEMG-Classifier/models/mobilenetv1.py:45:0\n",
      "  %/Reshape_output_0 : Float(1, 128, strides=[128, 1], requires_grad=1, device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"/Reshape\"](%/model/model.7/GlobalAveragePool_output_0, %/Constant_output_0), scope: models.mobilenetv1.MobilenetV1:: # /home/studenta/Documents/CNN-HD-sEMG-Classifier/models/mobilenetv1.py:45:0\n",
      "  %123 : Float(1, 8, strides=[8, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"/fc/Gemm\"](%/Reshape_output_0, %fc.weight, %fc.bias), scope: models.mobilenetv1.MobilenetV1::/torch.nn.modules.linear.Linear::fc # /home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/torch/nn/modules/linear.py:114:0\n",
      "  return (%123)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from models.mobilenetv1 import MobilenetV1\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dummy_input = torch.randn(1, 1, 8, 24, device=device)\n",
    "model = MobilenetV1(ch_in=1, n_classes=8).to(device)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"pretrain_model/MobilenetV1_Param@29.29 k _MAC@233.1 KMac_Acc@95.346.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# print(model)\n",
    "torch.onnx.export(model, dummy_input,\n",
    "                  \"pretrain_model/onnx_model/MobilenetV1.onnx\", verbose=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conver Onnx Model to Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install library\n",
    "%cd onnx2keras\n",
    "!pip install -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:21:53.044613: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 11:21:53.243317: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 11:21:53.870976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-06-13 11:21:54.756227: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2023-06-13 11:21:54.756250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: cose-zwqin-r13\n",
      "2023-06-13 11:21:54.756254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: cose-zwqin-r13\n",
      "2023-06-13 11:21:54.756287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 515.105.1\n",
      "2023-06-13 11:21:54.756298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 515.43.4\n",
      "2023-06-13 11:21:54.756300: E tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:312] kernel version 515.43.4 does not match DSO version 515.105.1 -- cannot find working devices in this configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input.1 (InputLayer)        [(None, 8, 24, 1)]        0         \n",
      "                                                                 \n",
      " LAYER_0_pad (ZeroPadding2D)  (None, 10, 26, 1)        0         \n",
      "                                                                 \n",
      " LAYER_0 (Conv2D)            (None, 4, 12, 32)         320       \n",
      "                                                                 \n",
      " LAYER_1 (Activation)        (None, 4, 12, 32)         0         \n",
      "                                                                 \n",
      " LAYER_2_pad (ZeroPadding2D)  (None, 6, 14, 32)        0         \n",
      "                                                                 \n",
      " LAYER_2 (DepthwiseConv2D)   (None, 4, 12, 32)         320       \n",
      "                                                                 \n",
      " LAYER_3 (Activation)        (None, 4, 12, 32)         0         \n",
      "                                                                 \n",
      " LAYER_4 (Conv2D)            (None, 4, 12, 32)         1056      \n",
      "                                                                 \n",
      " LAYER_5 (Activation)        (None, 4, 12, 32)         0         \n",
      "                                                                 \n",
      " LAYER_6_pad (ZeroPadding2D)  (None, 6, 14, 32)        0         \n",
      "                                                                 \n",
      " LAYER_6 (DepthwiseConv2D)   (None, 2, 6, 32)          320       \n",
      "                                                                 \n",
      " LAYER_7 (Activation)        (None, 2, 6, 32)          0         \n",
      "                                                                 \n",
      " LAYER_8 (Conv2D)            (None, 2, 6, 64)          2112      \n",
      "                                                                 \n",
      " LAYER_9 (Activation)        (None, 2, 6, 64)          0         \n",
      "                                                                 \n",
      " LAYER_10_pad (ZeroPadding2D  (None, 4, 8, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " LAYER_10 (DepthwiseConv2D)  (None, 2, 6, 64)          640       \n",
      "                                                                 \n",
      " LAYER_11 (Activation)       (None, 2, 6, 64)          0         \n",
      "                                                                 \n",
      " LAYER_12 (Conv2D)           (None, 2, 6, 64)          4160      \n",
      "                                                                 \n",
      " LAYER_13 (Activation)       (None, 2, 6, 64)          0         \n",
      "                                                                 \n",
      " LAYER_14_pad (ZeroPadding2D  (None, 4, 8, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " LAYER_14 (DepthwiseConv2D)  (None, 1, 3, 64)          640       \n",
      "                                                                 \n",
      " LAYER_15 (Activation)       (None, 1, 3, 64)          0         \n",
      "                                                                 \n",
      " LAYER_16 (Conv2D)           (None, 1, 3, 64)          4160      \n",
      "                                                                 \n",
      " LAYER_17 (Activation)       (None, 1, 3, 64)          0         \n",
      "                                                                 \n",
      " LAYER_18_pad (ZeroPadding2D  (None, 3, 5, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " LAYER_18 (DepthwiseConv2D)  (None, 1, 3, 64)          640       \n",
      "                                                                 \n",
      " LAYER_19 (Activation)       (None, 1, 3, 64)          0         \n",
      "                                                                 \n",
      " LAYER_20 (Conv2D)           (None, 1, 3, 64)          4160      \n",
      "                                                                 \n",
      " LAYER_21 (Activation)       (None, 1, 3, 64)          0         \n",
      "                                                                 \n",
      " LAYER_22_pad (ZeroPadding2D  (None, 3, 5, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " LAYER_22 (DepthwiseConv2D)  (None, 1, 2, 64)          640       \n",
      "                                                                 \n",
      " LAYER_23 (Activation)       (None, 1, 2, 64)          0         \n",
      "                                                                 \n",
      " LAYER_24 (Conv2D)           (None, 1, 2, 128)         8320      \n",
      "                                                                 \n",
      " LAYER_25 (Activation)       (None, 1, 2, 128)         0         \n",
      "                                                                 \n",
      " LAYER_26 (GlobalAveragePool  (None, 128)              0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " LAYER_26_EXPAND1 (Lambda)   (None, 128, 1)            0         \n",
      "                                                                 \n",
      " LAYER_26_EXPAND2 (Lambda)   (None, 128, 1, 1)         0         \n",
      "                                                                 \n",
      " LAYER_28 (Reshape)          (None, 128)               0         \n",
      "                                                                 \n",
      " LAYER_29 (Dense)            (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,520\n",
      "Trainable params: 28,520\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"pretrain_model/onnx_model/MobilenetV1.onnx\")\n",
    "from onnx2keras import onnx_to_keras\n",
    "model = onnx_to_keras(onnx_model, ['input.1'],name_policy='renumerate',verbose=False,change_ordering=True)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studenta/anaconda3/envs/sEMG/lib/python3.9/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize.py:216: UserWarning: Lambda layers are not supported by automatic model annotation because the internal functionality cannot always be determined by serialization alone. We recommend that you make a custom layer and add a custom QuantizeConfig for it instead. This layer will not be quantized which may lead to unexpected results.\n",
      "  warnings.warn(\n",
      "Processing Files: 100%|██████████| 5/5 [00:00<00:00,  8.06it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:00<00:00,  6.87it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:00<00:00,  6.24it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:00<00:00,  5.78it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:00<00:00,  5.10it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:01<00:00,  4.84it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:01<00:00,  4.33it/s]\n",
      "Processing Files: 100%|██████████| 5/5 [00:01<00:00,  3.53it/s]\n",
      "Processing Files: 100%|██████████| 8/8 [00:07<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "824/824 [==============================] - 31s 36ms/step - loss: 0.1779 - accuracy: 0.9396\n",
      "Epoch 2/2\n",
      "824/824 [==============================] - 32s 39ms/step - loss: 0.0957 - accuracy: 0.9668\n",
      "406/406 [==============================] - 5s 11ms/step - loss: 0.0839 - accuracy: 0.9709\n",
      "Quant test accuracy: 0.970863401889801\n"
     ]
    }
   ],
   "source": [
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(model)\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from utils.ICE_lab_data_preprocessing import ICE_lab_data_preprocessing as utils\n",
    "\n",
    "data,label,num_classes = utils().extra_data(\"data/Training_Trimmed\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data, testing_data, training_label, testing_label = train_test_split(data, label, test_size=0.33, random_state=42)\n",
    "train_data = tf.data.Dataset.from_tensor_slices((training_data, training_label))\n",
    "test_data = tf.data.Dataset.from_tensor_slices((testing_data, testing_label))\n",
    "\n",
    "training_data = training_data.reshape(-1,8,24,1)\n",
    "testing_data = testing_data.reshape(-1,8,24,1)\n",
    "training_data = utils().NormalizeData(training_data)\n",
    "testing_data = utils().NormalizeData(testing_data)\n",
    "q_aware_model.fit(training_data,training_label,\n",
    "                  batch_size=1000, epochs=2)\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "   testing_data, testing_label, batch_size=1000,verbose=True)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:23:43.168834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128,1,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-13 11:23:43.171245: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128,1,1]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) input.1 with unsupported characters which will be renamed to input_1 in the SavedModel.\n",
      "WARNING:absl:`input.1` is not a valid tf.function parameter name. Sanitizing to `input_1`.\n",
      "WARNING:absl:`input.1` is not a valid tf.function parameter name. Sanitizing to `input_1`.\n",
      "2023-06-13 11:23:44.109857: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128,1,1]\n",
      "\t [[{{node inputs}}]]\n",
      "2023-06-13 11:23:44.112012: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,128,1,1]\n",
      "\t [[{{node inputs}}]]\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla, LAYER_0_layer_call_fn, LAYER_0_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, LAYER_1_layer_call_fn while saving (showing 5 of 70). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pretrain_model/q_ware_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pretrain_model/q_ware_model/assets\n"
     ]
    }
   ],
   "source": [
    "q_aware_model.save(\"pretrain_model/q_ware_model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert it to Tensorflow Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:25:01.663078: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-06-13 11:25:01.663097: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-06-13 11:25:01.663224: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: pretrain_model/q_ware_model\n",
      "2023-06-13 11:25:01.667615: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-06-13 11:25:01.667628: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: pretrain_model/q_ware_model\n",
      "2023-06-13 11:25:01.681657: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-06-13 11:25:01.793021: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: pretrain_model/q_ware_model\n",
      "2023-06-13 11:25:01.826286: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 163061 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def representative_dataset():\n",
    "    data = np.load(\"representive_data.npy\")\n",
    "    for i in range(1):\n",
    "        temp_data = data[i]\n",
    "        temp_data = temp_data.reshape(1,8,24,1)\n",
    "        yield [temp_data.astype(np.float32)]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"pretrain_model/q_ware_model\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.float32  # or tf.uint8\n",
    "converter.inference_output_type = tf.float32\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open(\"pretrain_model/tf_lite_model/mobilenetv1.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sample Size: 30720\n",
      "Prediction Correct Size: 28471\n",
      "Accuracy 0.93\n",
      "Total Sample Size: 30720\n",
      "Prediction Correct Size: 29850\n",
      "Accuracy 0.97\n",
      "Total Sample Size: 30720\n",
      "Prediction Correct Size: 27100\n",
      "Accuracy 0.88\n",
      "Total Sample Size: 30720\n",
      "Prediction Correct Size: 29913\n",
      "Accuracy 0.97\n",
      "Total Sample Size: 30720\n",
      "Prediction Correct Size: 34\n",
      "Accuracy 0.0\n",
      "Total Sample Size: 30720\n",
      "Prediction Correct Size: 26710\n",
      "Accuracy 0.87\n",
      "Total Sample Size: 30880\n",
      "Prediction Correct Size: 8203\n",
      "Accuracy 0.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Load the TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path=\"pretrain_model/tf_lite_model/mobilenetv1.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data\n",
    "input_shape = input_details[0]['shape']\n",
    "# input_data = np.array(np.random.random_sample(input_shape), dtype=np.int8)\n",
    "for j in range(2,9):\n",
    "    ori_input_data = np.load(f\"representive_data{j}.npy\")\n",
    "    ori_input_data = ori_input_data.astype(np.float32)\n",
    "    # ori_input_data = ori_input_data.reshape(-1,8,24,1)\n",
    "    correct = 0\n",
    "    print(\"Total Sample Size:\",ori_input_data.shape[0])\n",
    "    for i in range(ori_input_data.shape[0]):\n",
    "        input_data = np.expand_dims(ori_input_data[i], 0)\n",
    "        input_data = input_data.reshape(-1,8,24,1)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # get_tensor() returns a copy of the tensor data\n",
    "        # use tensor() in order to get a pointer to the tensor\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        if np.argmax(output_data) == j-1:\n",
    "            correct += 1\n",
    "    print(\"Prediction Correct Size:\",correct) #Total:30720\n",
    "    print(\"Accuracy\",round(correct/int(ori_input_data.shape[0]),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sEMG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
